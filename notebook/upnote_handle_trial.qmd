---
title: "RMeCabのテスト:Upnote"
date: today
---

## Collect upnote text

UpNoteのメモをマークダウンに出力したものを使う。

```{r}
# library(tm)
library(RMeCab)
library(tidyverse)
```

作成日等のメタデータを含むので、それを解析する。

```{r}
read_upnote_text <- function(fpath){
  
  fname = basename(fpath)
  d1 <- read_lines(fpath, skip = 1)
  endl <- length(d1)

  lno <-  1
  lbdy <- 1
  isheader <- TRUE
  cat_mode <-  FALSE
  title_str <-  "" # default
  tags = c()
  bdytxt = c()

  for (l in d1){
    lno = lno + 1
    if(isheader){
      if( str_detect(l, "date:")){
        update_dt <- as_datetime(str_remove(l,"date: "), tz = Sys.timezone())
      }else if( str_detect(l, "created:")){
        create_dt <- as_datetime(str_remove(l,"created: "), tz = Sys.timezone())
      }else if(str_detect(l,"categories:")){
        cat_mode <-  TRUE
        cc = c()
      }else if(str_detect(l,"---")){
        cat_mode <-  FALSE 
        isheader <- FALSE # end of header lines
        next
      }else{
        if(cat_mode){
          cc = c(cc, str_remove(l,"^- "))
        } 
      } 
    }else{ # body
      if(str_detect(l,"^#+ ")){
        # handle headers
        bdytxt = c(bdytxt, str_remove(l,"^#+ "))
      }else if(str_detect(l,"^[\\s\\*]+$")){
        # ignore ruler line
        bdytxt = c(bdytxt, str_replace(l,"^[\\s\\*]+$",""))
      }else if(str_detect(l,"^#[^[:punct:] ]*$")){
        # search tag lines
        tags <-  c(tags,str_remove(l,"^#"))
      }else{
        bdytxt = c(bdytxt, l)
      }
    }
  }
    
  contents <- str_c(bdytxt, collapse = "\n") # contents without headers and tags
  
  if(str_length(title_str) == 0 ){
    title_str <- str_remove(fpath, ".md$")
  }
  
  catstr <- str_c(cc, collapse = "|")
  tagstr <- str_c(tags, collapse = "|")
  
  return(tibble(category = catstr, contents = contents,
                created = create_dt, 
                fpath = fname, tags = tagstr, update = update_dt))
}
```

```{r}
file1 <- "../UpNote/General Space/1番管取付胴輪 2.md"
```

```{r}
d1 <- read_upnote_text(file1)
d1
```
```{r}
d1$contents |> cat()
```


```{r}
d2 <- read_upnote_text("UpNote/General Space/{gitlabr} v2.0 is on CRAN!  R-bloggers.md")
d2
```
```{r}
d3 <- read_upnote_text("UpNote/General Space/tidymodels.md")
d3$contents
```

上記の関数を全てのファイルに対して実行して、データフレームを返す。
時間のかかる処理になるだろう。

```{r}
all_files <- list.files("UpNote/General Space/", pattern = "*.md", full.names = TRUE)
```

```{r}
#| eval: false

# dfm <- all_files |> map(read_upnote_text) |> list_rbind()
```

結果を出力しておく。

```{r}
#| eval: false

# dfm |> write_csv("upnote_text.csv")
```

# RMeCab

RMeCabを使って元文を単語に分けて分かち書きをする。

改めてデータを読み込む。

```{r}
dfm_txt <- read_csv("upnote_text.csv")
```

```{r}
dfm1 <- dfm_txt |> 
  select(fpath, contents) |> 
  filter(!is.na(contents))  
```

コンテンツをRMeCabで分割する。
tidytextのunnest_tokensにRMeCabを割り当てることができる。

予めNElogd辞書を登録しておくことで、適切な分割が出来るようになっている。

```{r}
dfm1 |> filter(fpath == "19カギ 調整治具.md") |> 
   pull(contents) |>
```


```{r}
"\n大久保氏からの質問に回答。\n\n片持ちでカギ管を支えるタイプの検査治具って、どうしてその形？\n\n→検査治具ではなく、調整治具。\n\n  \n\n最初は検査治具の図面を片っ端から見ていて、参考図として取り上げられている調整治具\n\nTS-J-14840\n\nが写真に酷似しているので、行き当たった。\n\n  \n\nそして、写真そのものは\n\nAS-J-6626であることが、細部形状の比較等でほぼ確信。\n\n同時期に検査治具が集成用として無い（タンポ皿＋カギ用なら6465がある）ので、集成検査治具兼用として設計されたかな。\n\n  \n\n類似品に19648、これは図番のみで空番。\n\n  \n\nそこから発展して、YMPIでカギの検査治具をどうしようかについて、TEAMSチャットでやりとり。" |> 

  tolower() |> 
  str_replace_all("([a-z])-([jd])-([0-9]{4,5})","\\1\\2\\3") |> 
  RMeCabC() |> unlist()
```


```{r}
|> 

|> 
```


```{r}
```

```{r}
dfm1 |> filter(fpath == "19カギ 調整治具.md") |> 
  unnest_tokens(words, contents, token = "regex", pattern = "([a-zA-Z]-[jdJD]-[0-9]+)")
```


```{r}
RMeCabC("DOS窓では、基本的には日本語がアウトです", dic = "ipadic-neologd") |> unlist()
```

名詞、動詞、形容詞以外は落とす。
スペース分割にすることでtidytextで処理できるようにする。

```{r}
pick_word <- function(txt){
  tt <- unlist(RMeCabC(txt))
  nm <- names(tt)
  
  return(str_c(tt[nm %in% c("名詞","動詞","形容詞")], collapse = " "))
}
```

```{r}
pick_word(dfm1$contents[1])
```

```{r}
dfm2 <- 
  dfm1 |> 
  rowwise() |> 
  mutate(txtsplit = pick_word(contents)) |> 
  select(-contents)
```

```{r}
dfm2 |> tail()
```

## Stopwords

```{r}
library(stopwords)
library(tidytext)
```

```{r}
mystopwords <- tibble( words = c(
  stopwords("ja", source = "marimo"),
  stopwords("en")
))
```

分かち書きされているデータを読む。

```{r}
dfm <- read_csv("upnote_text_split.csv")
```
```{r}
dfm1 <- 
  dfm |> 
  # head(5) |> 
  select(fpath, contents_split) |> 
  separate_rows(contents_split, sep = " ") |> 
  rename(words = contents_split)

```


```{r}
dfm2 <- 
  dfm1 |> anti_join(mystopwords, by = c("words"))
```


```{r}
dfm2 |> group_by(words) |> count() |> 
  arrange(desc(n)) |> head(50)
```

オリジナルのストップワードを作る。
特定の用語はVSCodeで用意。%や数字などは記号判定などで落とす予定。

先頭50を取り出してファイルに書き出してから編集。

```{r}
#| eval: false
# dfm2 |> group_by(words) |> count() |> 
  # arrange(desc(n)) |> head(50) |> pull(words) |> write_lines("mystopwords.txt")
```


```{r}
sword <- tibble(words = read_lines("mystopwords.txt"))
```

```{r}
dfm3 <- 
  dfm2 |> anti_join(sword, by = c("words"))

```

```{r}
dfm3 |> 
  filter(!str_detect(words, "-?\\d+\\.?\\d*(?:[eE][-+]?\\d+)?")) |> # numeric words
  filter(!str_detect(words, "[A-Za-z]")) |> # single alpha
  group_by(words) |> 
  count() |> 
  arrange(desc(n)) |> 
  head(50)
```

## TF-IDF

別途処理済みのキーワードを読み込む。

```{r}
dfm <- read_csv("upnote_keywords.csv")
```
用語トップ

```{r}
dfm |> group_by(words) |> count() |> 
  arrange(desc(n)) |> 
  head(50)
```

TF-IDFの計算の為、1文書中の出現回数を計算する。
twdは1文書の単語数。

簡単なチェックのため最初の4文書を取り出すコードを挟んである。
コメントアウトすれば全文書が対象になる。

```{r}
docs <- dfm |> group_by(fpath) |> count() |> 
  head(4) |> pull(fpath)
```


```{r}
dfm1 <- 
  dfm |> 
    filter(fpath %in% docs) |>
    # head(1000) |> 
    count(fpath, words) |>
    group_by(fpath) |> 
    mutate(nwords = sum(n)) 

dfm1
```

tidytextにtf-idfを計算する関数がある。

```{r}
dfm_tfidf <- 
  dfm1 |> 
  bind_tf_idf(words, fpath, n)

dfm_tfidf
```

```{r}
#| fig-width: 16
#| fig-height: 9

dfm_tfidf %>%
  group_by(fpath) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(words, tf_idf), fill = fpath)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ fpath, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

## word cloud

```{r}
dfm <- read_csv("upnote_tfidf.csv")
```
用語が10以下しか含まれない短文は除外する。

```{r}
dfm |> nrow()
```
```{r}
dfm |> filter(nwords > 10) |> nrow()
```


```{r}
dfm |> 
  filter(!str_detect(fpath,"業務時間") ) |> 
  filter(nwords > 10) |> 
  arrange(desc(tf_idf)) |> head(10)
```

```{r}
library(wordcloud2)
```
tf_idfのトップ100についてのワードクラウド。

```{r}
dfm |> 
  filter(!str_detect(fpath,"業務時間") ) |> 
  filter( nwords > 10) |> 
  slice_max(tf_idf, n = 100) |> 
  select(word = words, freq = tf_idf) |> 
  wordcloud2()

```

うーん、しょうもないワードしか出てこない。

## doc similarity

widyr のpairwise_similarityで文書間の類似度を測定できる。

```{r}
library(widyr)
```
相当時間がかかる。

```{r}
dfm_ps <- 
  dfm |> 
  filter(fpath %in% docs) |>
  pairwise_similarity(fpath, words, tf_idf) |> 
  arrange(desc(similarity))

dfm_ps
```
## DTM

```{r}
library(tm)
```
```{r}

```


```{r}
corpus <- Corpus(VectorSource(filter(dfm, fpath %in% docs)$fpath))
```

```{r}
dtm <- DocumentTermMatrix(corpus)
```

```{r}
dtm
```
## Topic models

```{r}
library(topicmodels)
```

トピックモデルを作成する。

まず文書にやたらとbrを入れているような業務時間のノートや、単語数が少なくてTF*IDFが大きくなってしまうようなデータを削除する。
WEB記事のクリップになっているやつはタイトルが長い。

```{r}
dfm_splt <- read_csv("upnote_text_split.csv")
```

```{r}
dfm_titles <- dfm_splt |> 
  select(fpath, title) |> 
  mutate(tlen = str_length(title)) |> 
  arrange(desc(tlen)) 
```

```{r}
dfm_titles |> group_by(tlen) |> 
  ggplot(aes(x = tlen)) + geom_bar() +
  labs(title = "文書タイトル長頻度", x = "タイトル長") 
```
タイトルの長さが25よりも長いものはほとんど存在してない。

```{r}
dfm_titles |> filter(tlen > 25)
```

ネット記事のクリップは分析から外したいが、うまく外す方法が思いつかない。

```{r}
dfm_tfidf <- read_csv("upnote_tfidf.csv")
```

TF_IDFを計算したデータ中でnwordsで頻度を見る。

```{r}
dfm_tfidf_w <- 
  dfm_tfidf |> 
    group_by(fpath) |> 
    summarise(fpath = first(fpath),nwords = first(nwords))
```

```{r}
dfm_tfidf_w |> ggplot(aes(x = nwords)) + 
  geom_bar()
```


```{r}
dfm_tfidf_w |>  
  filter(nwords > 1000)
```

両方の項目を使うとネット資料を排除できるかも。
タイトルが長すぎて内容の単語も長いものは削除する。

```{r}
dfm_elim1 <- 
  dfm_tfidf_w |> 
    left_join(dfm_titles, by = "fpath") |> 
    filter((nwords > 1000) & (tlen > 32))
```

```{r}
dfm_elim1
```

```{r}
dfm_tfidf_w |> ggplot(aes(x = nwords)) + 
  geom_bar() + 
  xlim(0,100)
```

```{r}
dfm_tfidf_w |> filter(nwords < 3)
```

```{r}
dfm_tfidf |> filter(fpath == "1番管仮寸切治具.md")
```

↑で試してみるとわかるがnwordsが1だと休暇とか一言入れてあるだけなので分析の意味がない。
またタイトルに業務時間を含むのは時間集計なので不要。

```{r}
dfm_elim2 <- 
  dfm_tfidf_w |> filter((nwords < 2) | str_detect(fpath,"業務時間")) 

# dfm_elim2
```


```{r}
dfm_remain <- dfm_tfidf |> 
  anti_join(dfm_elim1, by = "fpath") |> 
  anti_join(dfm_elim2, by = "fpath")
```

DTMを作成する。

```{r}
dtm_remain <- 
  dfm_remain |> cast_dtm(fpath, words, n)
```


```{r}
dtm_remain
```

LDAを実施してみる。時間がかかるよ。それでも数分で終わる。

```{r}
dtm_lda <- LDA(dtm_remain, k = 5, control = list(seed = "10"))
```

```{r}
dtm_topics <- tidy(dtm_lda)
```

各トピック毎にどういう単語がよく出るか、で、トピックの傾向が判定できる。

```{r}
dtm_topics |> 
  group_by(topic) |> 
  slice_max(beta, n = 10) 
```

2番目はgit関係なのが明らか。3番目はHR関係か。

augmentを使うことで、ドキュメントがどのトピックに割り当てられるかがわかる。

```{r}
dfm_assgn <- augment(dtm_lda, dtm_remain)
```

```{r}
dfm_assgn
```

```{r}
dfm_assgn_doc <- 
  dfm_assgn |> group_by(document) |> 
    summarise(topic_m = mean(.topic))
```

```{r}
dfm_assgn_doc
```


```{r}
dfm_txt <- read_csv("upnote_text.csv")
```
```{r}
dfm_cmp <- dfm_txt |> 
  inner_join(dtm_topics, by = c("fpath" = "term"))
```

```{r}
dfm_cmp |> 
  ggplot(aes(x = fpath, y = beta, fill = as.factor(topic))) + 
  geom_col(position = "dodge")
```

```{r}
dfm_txt |> filter(fpath == "0030.md") |> pull(contents) |> cat()
```
```{r}
dfm_txt |> filter(fpath == "1,2番管.md") |> pull(contents) |> cat()
```


## etc

